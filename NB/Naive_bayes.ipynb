{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show the code of naive bayes\n",
    "\n",
    "贝叶斯就是把无法计算的概率，转化为容易计算的概率，然后算出来的过程。中间需要一些假设，而且即使这些假设不成立，分类效果依旧很好。\n",
    "\n",
    "- 离散型数据\n",
    "\n",
    "- 连续型数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes - 离散型数据（垃圾邮件分类模型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本例是一个帖子分类问题。输入评论的句子，判断帖子是否有侮辱性。--> 再复杂点就是垃圾邮件分类。\n",
    "\n",
    "对于单词的统计有2种策略，1是只统计有无出现，2是统计出现的次数。\n",
    "\n",
    "\n",
    "- 从文本，构建文本所有单词列表。\n",
    "\n",
    "- 然后构建每个文本中是否出现该单词，1出现，0没有出现。\n",
    "\n",
    "- 计算每个单词出现的频率: w表示词向量， ci表示帖子分类\n",
    "\n",
    "- 训练模型就是计算每个分类下，每个单词的频率。\n",
    "\n",
    "- 测试模型，就是输入单词次数，使用计算好的各个分类下的单词频率，计算频率最高的分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       "  ['mybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       "  ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       "  ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       "  ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       "  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']],\n",
       " [0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def loadDataSet():\n",
    "    postingList=[\n",
    "        'my dog has flea problems help please',\n",
    "        'mybe not take him to dog park stupid',\n",
    "        'my dalmation is so cute I love him',\n",
    "        'stop posting stupid worthless garbage',\n",
    "        'mr licks ate my steak how to stop him',\n",
    "        'quit buying worthless dog food stupid'\n",
    "    ]\n",
    "    classVec=[0,1,0,1,0,1] #是否有侮辱性词汇，1有，0没；\n",
    "    postingList2=[]\n",
    "    for s in postingList:\n",
    "        postingList2.append( re.split(' ',s) )\n",
    "    return postingList2, classVec\n",
    "# test\n",
    "loadDataSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'park', 'ate', 'buying', 'problems', 'dalmation', 'is', 'mybe', 'dog', 'not', 'him', 'posting', 'stop', 'flea', 'worthless', 'take', 'please', 'to', 'quit', 'my', 'so', 'cute', 'I', 'garbage', 'food', 'steak', 'licks', 'stupid', 'love', 'mr', 'has', 'help']\n"
     ]
    }
   ],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet=set()\n",
    "    for words in dataSet:\n",
    "        vocabSet=vocabSet | set(words) # 求并集\n",
    "    return list(vocabSet) #集合 to list\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    rsVect=[0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            rsVect[vocabList.index(word)]=1\n",
    "        else:\n",
    "            print('the word: %s is not in my Vocabulary!' % word) #这个应该用不到\n",
    "    return rsVect\n",
    "\n",
    "# test\n",
    "posts,tags=loadDataSet()\n",
    "vocabList=createVocabList(posts)\n",
    "print(vocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print( setOfWords2Vec(vocabList, posts[0]) )\n",
    "print( setOfWords2Vec(vocabList, posts[2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# get word vector matrix\n",
    "train_X=[]\n",
    "for post in posts:\n",
    "    train_X.append(setOfWords2Vec(vocabList, post))\n",
    "print(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get prob from word vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.由词向量，我们知道一个词是否出现在某个文档中，也知道了某个文档的分类。\n",
    "\n",
    "* p(ci|w) = p(w|ci)*p(ci) / p(w)\n",
    "* 输入句子，判断属于什么分类 p(ci|w)的问题，就变成计算\n",
    "* 出现某个分类的概率p(ci),和该分类中某个单词的频率 p(w|ci).\n",
    "\n",
    "2.而计算 p(w|ci)=p(w0,w1,...,wn|ci) 就是统计该分类下，每个单词出现的概率。\n",
    "* 这里要使用朴素贝叶斯的假设了: 各个条件相互独立，则 \n",
    "* p(w|ci)=p(w0,w1,...,wn|ci)=p(w0|ci)p(w1|ci)*...*p(wn|ci)\n",
    "* 这样就极大的简化了计算过程。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def trainNB0(train_X, train_Y):\n",
    "    numTrainDocs=len(train_X)\n",
    "    # 单词总数\n",
    "    numWords=len(train_X[0])\n",
    "    # 1分类的百分比\n",
    "    pAbusive=sum(train_Y)/float(numTrainDocs)\n",
    "    \n",
    "    p0Num=np.zeros(numWords); p1Num=np.zeros(numWords);\n",
    "    p0Denom=0.0; p1Denom=0.0;\n",
    "    for i in range(numTrainDocs):\n",
    "        if train_Y[i]==1:\n",
    "            p1Num += train_X[i] #该分类下，每个单词的数量\n",
    "            p1Denom += sum(train_X[i]) #该分类下，总单词数\n",
    "        else:\n",
    "            p0Num += train_X[i]\n",
    "            p0Denom += sum(train_X[i])\n",
    "    p1Vect=p1Num/p1Denom; # num ro frequncy\n",
    "    p0Vect=p0Num/p0Denom;\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "# test\n",
    "pV0,pV1,pAb=trainNB0(train_X, train_Y=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04166667 0.         0.04166667 0.         0.04166667 0.04166667\n",
      " 0.04166667 0.         0.04166667 0.         0.08333333 0.\n",
      " 0.04166667 0.04166667 0.         0.         0.04166667 0.04166667\n",
      " 0.         0.125      0.04166667 0.04166667 0.04166667 0.\n",
      " 0.         0.04166667 0.04166667 0.         0.04166667 0.04166667\n",
      " 0.04166667 0.04166667]\n",
      "[0.         0.05263158 0.         0.05263158 0.         0.\n",
      " 0.         0.05263158 0.10526316 0.05263158 0.05263158 0.05263158\n",
      " 0.05263158 0.         0.10526316 0.05263158 0.         0.05263158\n",
      " 0.05263158 0.         0.         0.         0.         0.05263158\n",
      " 0.05263158 0.         0.         0.15789474 0.         0.\n",
      " 0.         0.        ]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(pV0)\n",
    "print(pV1)\n",
    "print(pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cute' 在0出现1次，1出现0次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据实际情况修改分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不能出现零，否则相乘后都是0.\n",
    "\n",
    "- 同时，x>0时，f(x)与 f(ln(x)) 的单调性相同。所以把乘法转为取log后的加法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936 -2.56494936\n",
      " -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.15948425 -3.25809654\n",
      " -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936 -2.56494936\n",
      " -3.25809654 -1.87180218 -2.56494936 -2.56494936 -2.56494936 -3.25809654\n",
      " -3.25809654 -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936\n",
      " -2.56494936 -2.56494936]\n",
      "[-3.04452244 -2.35137526 -3.04452244 -2.35137526 -3.04452244 -3.04452244\n",
      " -3.04452244 -2.35137526 -1.94591015 -2.35137526 -2.35137526 -2.35137526\n",
      " -2.35137526 -3.04452244 -1.94591015 -2.35137526 -3.04452244 -2.35137526\n",
      " -2.35137526 -3.04452244 -3.04452244 -3.04452244 -3.04452244 -2.35137526\n",
      " -2.35137526 -3.04452244 -3.04452244 -1.65822808 -3.04452244 -3.04452244\n",
      " -3.04452244 -3.04452244]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "import numpy as np\n",
    "def trainNB1(train_X, train_Y):\n",
    "    numTrainDocs=len(train_X)\n",
    "    # 单词总数\n",
    "    numWords=len(train_X[0])\n",
    "    # 1分类的百分比\n",
    "    pAbusive=sum(train_Y)/float(numTrainDocs)\n",
    "    \n",
    "    p0Num=np.ones(numWords); p1Num=np.ones(numWords);\n",
    "    p0Denom=2.0; p1Denom=2.0;\n",
    "    for i in range(numTrainDocs):\n",
    "        if train_Y[i]==1:\n",
    "            p1Num += train_X[i] #该分类下，每个单词的数量\n",
    "            p1Denom += sum(train_X[i]) #该分类下，总单词数\n",
    "        else:\n",
    "            p0Num += train_X[i]\n",
    "            p0Denom += sum(train_X[i])\n",
    "    p1Vect=np.log(p1Num/p1Denom) # num ro frequncy\n",
    "    p0Vect=np.log(p0Num/p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "# test\n",
    "pV0,pV1,pAb=trainNB1(train_X, train_Y=tags)\n",
    "print(pV0)\n",
    "print(pV1)\n",
    "print(pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1=sum(vec2Classify*p1Vec) + np.log(pClass1)\n",
    "    p0=sum(vec2Classify*p0Vec) + np.log(1.0-pClass1)\n",
    "    print(p0, p1)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "-7.694848072384611 -9.826714493730215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entryPost=['love', 'my', 'dalmation']\n",
    "entryVect=np.array(setOfWords2Vec(vocabList, entryPost))\n",
    "print( entryVect )\n",
    "classifyNB(entryVect, pV0, pV1, pAb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
      "-7.20934025660291 -4.702750514326955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entryPost=['stupid', 'garbage']\n",
    "entryVect=np.array(setOfWords2Vec(vocabList, entryPost))\n",
    "print( entryVect )\n",
    "classifyNB(entryVect, pV0, pV1, pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词集模型(词是否出现)；词袋模型(词出现的次数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords2Vec(vocabList, inputSet):\n",
    "    rsVec=[0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            rsVec[vocabList.index(word)] += 1\n",
    "    return rsVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]]\n",
      "[-2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.56494936 -2.56494936\n",
      " -2.56494936 -3.25809654 -2.56494936 -3.25809654 -2.15948425 -3.25809654\n",
      " -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936 -2.56494936\n",
      " -3.25809654 -1.87180218 -2.56494936 -2.56494936 -2.56494936 -3.25809654\n",
      " -3.25809654 -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936\n",
      " -2.56494936 -2.56494936]\n",
      "[-3.04452244 -2.35137526 -3.04452244 -2.35137526 -3.04452244 -3.04452244\n",
      " -3.04452244 -2.35137526 -1.94591015 -2.35137526 -2.35137526 -2.35137526\n",
      " -2.35137526 -3.04452244 -1.94591015 -2.35137526 -3.04452244 -2.35137526\n",
      " -2.35137526 -3.04452244 -3.04452244 -3.04452244 -3.04452244 -2.35137526\n",
      " -2.35137526 -3.04452244 -3.04452244 -1.65822808 -3.04452244 -3.04452244\n",
      " -3.04452244 -3.04452244]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "import numpy as np\n",
    "def trainNB2(train_X, train_Y):\n",
    "    numTrainDocs=len(train_X)\n",
    "    # 单词总数\n",
    "    numWords=len(train_X[0])\n",
    "    # 1分类的百分比\n",
    "    pAbusive=sum(train_Y)/float(numTrainDocs)\n",
    "    \n",
    "    p0Num=np.ones(numWords); p1Num=np.ones(numWords);\n",
    "    p0Denom=2.0; p1Denom=2.0;\n",
    "    for i in range(numTrainDocs):\n",
    "        if train_Y[i]==1:\n",
    "            p1Num += train_X[i] #该分类下，每个单词的数量\n",
    "            p1Denom += sum(train_X[i]) #该分类下，总单词数\n",
    "        else:\n",
    "            p0Num += train_X[i]\n",
    "            p0Denom += sum(train_X[i])\n",
    "    p1Vect=np.log(p1Num/p1Denom) # num ro frequncy\n",
    "    p0Vect=np.log(p0Num/p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "# test\n",
    "\n",
    "# get word vector matrix\n",
    "train_X2=[]\n",
    "for post in posts:\n",
    "    train_X2.append(bagOfWords2Vec(vocabList, post))\n",
    "print(train_X2)\n",
    "\n",
    "\n",
    "pV0_2,pV1_2,pAb_2=trainNB2(train_X2, train_Y=tags)\n",
    "print(pV0_2)\n",
    "print(pV1_2)\n",
    "print(pAb_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "-7.694848072384611 -9.826714493730215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entryPost=['love', 'my', 'dalmation']\n",
    "entryVect=np.array(bagOfWords2Vec(vocabList, entryPost))\n",
    "print( entryVect )\n",
    "classifyNB(entryVect, pV0_2, pV1_2, pAb_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
      "-7.20934025660291 -4.702750514326955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entryPost=['stupid', 'garbage']\n",
    "entryVect=np.array(bagOfWords2Vec(vocabList, entryPost))\n",
    "print( entryVect )\n",
    "classifyNB(entryVect, pV0_2, pV1_2, pAb_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 接着还有多分类(n>2)怎么处理?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 连续型变量怎么处理？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 比如iris数据是连续的，没有那么明确的分类，怎么使用贝叶斯分类呢？\n",
    "\n",
    "> https://blog.csdn.net/tortelee/article/details/79484735\n",
    "\n",
    "> https://blog.csdn.net/BlueBlueSkyZ/article/details/101562427\n",
    "\n",
    "> http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "   性别　　身高（英尺）　体重（磅）　　脚掌（英寸）\n",
    "　　男 　　　6 　　　　　　180　　　　　12\n",
    "　　男 　　　5.92　　　　　190　　　　　11\n",
    "　　男 　　　5.58　　　　　170　　　　　12\n",
    "　　男 　　　5.92　　　　　165　　　　　10\n",
    "　　女 　　　5 　　　　　　100　　　　　6\n",
    "　　女 　　　5.5 　　　　　150　　　　　8\n",
    "　　女 　　　5.42　　　　　130　　　　　7\n",
    "　　女 　　　5.75　　　　　150　　　　　9\n",
    "\n",
    "已知某人身高6英尺、体重130磅，脚掌8英寸，请问该人是男是女？"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "######\n",
    "P(ci|w)=P(w|ci)*p(ci) / p(w), 我们可以忽略掉p(w)，因为它是一个恒定的频率。\n",
    "而p(ci)很容易计算。\n",
    "根据朴素贝叶斯假设，条件都是相对独立的，p(w|ci)=p(w0|ci)p(w1|ci)p(w2|ci)...p(wn|ci)\n",
    "\n",
    "\n",
    "######\n",
    "根据概率密度函数，计算概率\n",
    "\n",
    "假设都是正态分布 X~ N(mu, sigma^2)，则需要计算正态分布的2个参数 \n",
    "\n",
    "heights=c(6,5.92,5.58,5.92)\n",
    "mean2=mean(heights);mean2 #5.855\n",
    "var2=var(heights);var2 #0.03503333\n",
    "# R\n",
    "1/sqrt(2*3.1415*0.035)*exp(-(6-5.855)^2/(2*0.035))\n",
    "\n",
    "## js\n",
    "1/Math.sqrt(2*3.1415*0.035)*Math.exp(-((6-5.855)**2)/(2*0.035)) #1.579206773964085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.579206773964085"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/np.sqrt(2*3.1415*0.035)*np.exp(-((6-5.855)**2)/(2*0.035))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 180, 12], [5.92, 190, 11], [5.58, 170, 12], [5.92, 165, 10], [5, 100, 6], [5.5, 150, 8], [5.42, 130, 7], [5.75, 150, 9]]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def loadDataSet2():\n",
    "    dataSet=[ #列名: 身高（英尺）　体重（磅）　　脚掌（英寸）\n",
    "        [6,180,12],\n",
    "        [5.92,190,11],\n",
    "        [5.58,170,12],\n",
    "        [5.92,165,10],\n",
    "        [5,100,6],\n",
    "        [5.5,150,8],\n",
    "        [5.42,130,7],\n",
    "        [5.75,150,9]\n",
    "    ]\n",
    "    tags=[1,1,1,1, 0,0,0,0] #tags 1=男的; 0=女的\n",
    "    return dataSet, tags\n",
    "dataSet, tags=loadDataSet2()\n",
    "print(dataSet)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get paras: trainBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniqTag= {0, 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [array([  5.4175, 132.5   ,   7.5   ]),\n",
       "  array([9.72250000e-02, 5.58333333e+02, 1.66666667e+00]),\n",
       "  4],\n",
       " 1: [array([  5.855, 176.25 ,  11.25 ]),\n",
       "  array([3.50333333e-02, 1.22916667e+02, 9.16666667e-01]),\n",
       "  4]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def trainBayes(dataSet, tags):\n",
    "    paras={}\n",
    "    uniqTag=set(tags)\n",
    "    print('uniqTag=',uniqTag)\n",
    "    for tag in uniqTag:\n",
    "        paras[tag]=[]\n",
    "        dataOfThisTag=[]\n",
    "        for i in range(len(tags)):\n",
    "            if tags[i]==tag:\n",
    "                dataOfThisTag.append(dataSet[i])\n",
    "        dataOfThisTag=np.array(dataOfThisTag)\n",
    "        # cal mean, var by column\n",
    "        paras[tag].append( dataOfThisTag.mean(axis=0))\n",
    "        paras[tag].append( dataOfThisTag.var(axis=0, ddof=1))\n",
    "        paras[tag].append( dataOfThisTag.shape[0] )\n",
    "    return paras\n",
    "# test\n",
    "paras=trainBayes(dataSet, tags)\n",
    "paras\n",
    "# 女: 三列的平均值，三列的方差, 观察条目个数\n",
    "# 男: 三列的平均值，三列的方差, 观察条目个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cal prob for a new entry"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "P(身高=6|男) x P(体重=130|男) x P(脚掌=8|男) x P(男)\n",
    "    = 6.1984 x e-9\n",
    "\n",
    "P(身高=6|女) x P(体重=130|女) x P(脚掌=8|女) x P(女)\n",
    "    = 5.3778 x e-4\n",
    "\n",
    "可以看到，女性的概率比男性要高出将近10000倍，所以判断该人为女性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -7.527996461433017\n",
      "1 -18.89914470021889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predictBayes(newEntry, paras):\n",
    "    newEntry=np.array(newEntry)\n",
    "    uniqTag=set(paras.keys())\n",
    "    rsDict={}\n",
    "    totalbyGroup={} # get pre-prob for each class\n",
    "    for tag in uniqTag:\n",
    "        rsDict[tag]=[]\n",
    "        totalbyGroup[tag]=paras[tag][-1]\n",
    "        # print(tag, paras[tag])\n",
    "        for col in range(len(paras[tag][0])):\n",
    "            mean1=paras[tag][0][col]\n",
    "            var1=paras[tag][1][col]\n",
    "            \n",
    "            X=newEntry[col]\n",
    "            p=1/np.sqrt(2*3.1415*var1)*np.exp(-((X-mean1)**2)/(2*var1))\n",
    "            rsDict[tag].append(p)\n",
    "    #\n",
    "    #print(rsDict)\n",
    "    # calc post-prob for each class\n",
    "    postProbs={}\n",
    "    for tag in uniqTag:\n",
    "        p0=totalbyGroup[tag]/sum(totalbyGroup.values())\n",
    "        p= np.sum(np.log( np.array( rsDict[tag] ) ) )+ np.log(np.array(p0)) #np把乘法转为加法，会更方便\n",
    "        print(tag, p)\n",
    "        # 记录后验概率最大的p和分类，并返回p最大的分类\n",
    "        if 'max' not in postProbs:\n",
    "            postProbs['max']=p\n",
    "            postProbs['tag']=tag\n",
    "        elif p>postProbs['max']:\n",
    "            postProbs['max']=p\n",
    "            postProbs['tag']=tag\n",
    "    return postProbs['tag']\n",
    "# test\n",
    "predictBayes([6, 130, 8],paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005378147104813586 6.197346005195689e-09 86781.4561314584\n"
     ]
    }
   ],
   "source": [
    "a0=np.exp(-7.527996461433017) #女\n",
    "a1=np.exp(-18.89914470021889) #男\n",
    "print(a0, a1, a0/a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数量太少了，只好用原始数据进行验证了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -15.542921834567345\n",
      "1 -4.800531449063267\n",
      ">>>>>>>>>> [6, 180, 12] ; Pred= 1 ; Actual= 1 ;  True\n",
      "0 -13.636833095951348\n",
      "1 -4.9998969370642765\n",
      ">>>>>>>>>> [5.92, 190, 11] ; Pred= 1 ; Actual= 1 ;  True\n",
      "0 -13.172573780545862\n",
      "1 -5.681484213984493\n",
      ">>>>>>>>>> [5.58, 170, 12] ; Pred= 1 ; Actual= 1 ;  True\n",
      "0 -9.82190772281702\n",
      "1 -5.563841467110501\n",
      ">>>>>>>>>> [5.92, 165, 10] ; Pred= 1 ; Actual= 1 ;  True\n",
      "0 -8.219747784529547\n",
      "1 -53.254230985347604\n",
      ">>>>>>>>>> [5, 100, 6] ; Pred= 0 ; Actual= 0 ;  True\n",
      "0 -6.086702033597909\n",
      "1 -14.499412403294258\n",
      ">>>>>>>>>> [5.5, 150, 8] ; Pred= 0 ; Actual= 0 ;  True\n",
      "0 -5.783074887763696\n",
      "1 -25.390624675999543\n",
      ">>>>>>>>>> [5.42, 130, 7] ; Pred= 0 ; Actual= 0 ;  True\n",
      "0 -7.220258217706933\n",
      "1 -9.858118397585406\n",
      ">>>>>>>>>> [5.75, 150, 9] ; Pred= 0 ; Actual= 0 ;  True\n"
     ]
    }
   ],
   "source": [
    "i=-1\n",
    "for item in dataSet:\n",
    "    i=i+1\n",
    "    pred=predictBayes(item,paras)\n",
    "    print('>>>>>>>>>>',item, '; Pred=',pred, '; Actual=', tags[i], '; ', pred==tags[i])\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test on iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\ML_MachineLearning\\\\NB'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n",
       "1           5.1          3.5           1.4          0.2  setosa\n",
       "2           4.9          3.0           1.4          0.2  setosa\n",
       "3           4.7          3.2           1.3          0.2  setosa\n",
       "4           4.6          3.1           1.5          0.2  setosa\n",
       "5           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def loadDataSet3():\n",
    "    return pd.read_csv('../iris_data/iris.csv', index_col=0)\n",
    "iris=loadDataSet3()\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 5)\n",
      "(30, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def splitData(df, test_ratio):\n",
    "    # 索引范围为[0, n), 随机选x个不重复\n",
    "    n=df.shape[0]\n",
    "    x=round(n*test_ratio)\n",
    "    index = np.random.choice(np.arange(n), size=x, replace=False)\n",
    "    #\n",
    "    test_index = np.array(index)\n",
    "    train_index = np.delete(np.arange(n), test_index)\n",
    "    return df.iloc[train_index,],df.iloc[test_index,]\n",
    "np.random.seed(1)\n",
    "train_set, test_set=splitData(iris, 0.2)\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np2vec(npArray):\n",
    "    arrayVec=[]\n",
    "    for i in range(npArray.shape[0]):\n",
    "        arrayVec.append([npArray.iloc[i,0], npArray.iloc[i,1],npArray.iloc[i,2],npArray.iloc[i,3]])\n",
    "    #\n",
    "    tags2=[]\n",
    "    for item in npArray['Species']:\n",
    "        tags2.append(item)\n",
    "    return arrayVec,tags2\n",
    "#test\n",
    "trainX, trainY=np2vec(train_set)\n",
    "testX, testY=np2vec(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniqTag= {'versicolor', 'virginica', 'setosa'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'setosa': [array([4.96153846, 3.36666667, 1.46666667, 0.23333333]),\n",
       "  array([0.11032389, 0.13385965, 0.02596491, 0.01122807]),\n",
       "  39],\n",
       " 'versicolor': [array([5.94594595, 2.73243243, 4.22972973, 1.30540541]),\n",
       "  array([0.28255255, 0.10947447, 0.21936937, 0.04052553]),\n",
       "  37],\n",
       " 'virginica': [array([6.525     , 2.95227273, 5.53409091, 2.02045455]),\n",
       "  array([0.39540698, 0.09418076, 0.31020613, 0.08073467]),\n",
       "  44]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras=trainBayes(trainX, tags=trainY)\n",
    "paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versicolor -38.21661432381203\n",
      "setosa 0.28235339339129295\n",
      "virginica -53.71715459357284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictBayes([4.9, 3.1, 1.5, 0.1],paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versicolor -44.12681589706359\n",
      "setosa -4.725057243408674\n",
      "virginica -58.4842691139446\n",
      ">>>>>>>>>> 0 [5.8, 4.0, 1.2, 0.2] ; Pred= setosa ; Actual= setosa ;  True \n",
      "\n",
      "versicolor -6.23331524726756\n",
      "setosa -80.23697354330952\n",
      "virginica -20.44157443119337\n",
      ">>>>>>>>>> 1 [5.1, 2.5, 3.0, 1.1] ; Pred= versicolor ; Actual= versicolor ;  True \n",
      "\n",
      "versicolor -2.01334834711614\n",
      "setosa -237.59636387441716\n",
      "virginica -5.666667398840877\n",
      ">>>>>>>>>> 2 [6.6, 3.0, 4.4, 1.4] ; Pred= versicolor ; Actual= versicolor ;  True \n",
      "\n",
      "versicolor -37.18421270508493\n",
      "setosa -2.3274555821143568\n",
      "virginica -52.71759580830226\n",
      ">>>>>>>>>> 3 [5.4, 3.9, 1.3, 0.4] ; Pred= setosa ; Actual= setosa ;  True \n",
      "\n",
      "versicolor -29.40310036990041\n",
      "setosa -646.109605523668\n",
      "virginica -8.607352700945972\n",
      ">>>>>>>>>> 4 [7.9, 3.8, 6.4, 2.0] ; Pred= virginica ; Actual= virginica ;  True \n",
      "\n",
      "versicolor -4.0208001079355515\n",
      "setosa -291.250757714522\n",
      "virginica -4.112394673839194\n",
      ">>>>>>>>>> 5 [6.3, 3.3, 4.7, 1.6] ; Pred= versicolor ; Actual= versicolor ;  True \n",
      "\n",
      "versicolor -16.911779620304813\n",
      "setosa -460.325929832495\n",
      "virginica -2.2716282172237214\n",
      ">>>>>>>>>> 6 [6.9, 3.1, 5.1, 2.3] ; Pred= virginica ; Actual= virginica ;  True \n",
      "\n",
      "versicolor -29.70967746466003\n",
      "setosa -4.26305828835927\n",
      "virginica -45.1224477481749\n",
      ">>>>>>>>>> 7 [5.1, 3.8, 1.9, 0.4] ; Pred= setosa ; Actual= setosa ;  True \n",
      "\n",
      "versicolor -35.33656523332123\n",
      "setosa 0.572611985453209\n",
      "virginica -51.19848902773319\n",
      ">>>>>>>>>> 8 [4.7, 3.2, 1.6, 0.2] ; Pred= setosa ; Actual= setosa ;  True \n",
      "\n",
      "versicolor -20.494030134778374\n",
      "setosa -551.055960649682\n",
      "virginica -2.222214324223158\n",
      ">>>>>>>>>> 9 [6.9, 3.2, 5.7, 2.3] ; Pred= virginica ; Actual= virginica ;  True \n",
      "\n",
      "versicolor -0.9718411960002352\n",
      "setosa -196.66539022178353\n",
      "virginica -8.69338570847679\n",
      ">>>>>>>>>> 10 [5.6, 2.7, 4.2, 1.3] ; Pred= versicolor ; Actual= versicolor ;  True \n",
      "\n",
      "versicolor -32.20679997407876\n",
      "setosa -2.840969095627875\n",
      "virginica -47.515776108518644\n",
      ">>>>>>>>>> 11 [5.4, 3.9, 1.7, 0.4] ; Pred= setosa ; Actual= setosa ;  True \n",
      "\n",
      "versicolor -17.585265145646748\n",
      "setosa -553.4969291329096\n",
      "virginica -1.875424229112653\n",
      ">>>>>>>>>> 12 [7.1, 3.0, 5.9, 2.1] ; Pred= virginica ; Actual= virginica ;  True \n",
      "\n",
      "versicolor -2.7498929609395093\n",
      "setosa -256.7347481527813\n",
      "virginica -4.936968939002526\n",
      ">>>>>>>>>> 13 [6.4, 3.2, 4.5, 1.5] ; Pred= versicolor ; Actual= versicolor ;  True \n",
      "\n",
      "versicolor -1.51998631771603\n",
      "setosa -252.95417148174593\n",
      "virginica -4.954447188880095\n",
      ">>>>>>>>>> 14 [6.0, 2.9, 4.5, 1.5] ; Pred= versicolor ; Actual= versicolor ;  True \n",
      "\n",
      "versicolor -40.62014736000626\n",
      "setosa -0.7390381509216919\n",
      "virginica -56.6466648556547\n",
      ">>>>>>>>>> 15 [4.4, 3.2, 1.3, 0.2] ; Pred= setosa ; Actual= setosa ;  True \n",
      "\n",
      "versicolor -1.128043327160095\n",
      "setosa -169.2010211660469\n",
      "virginica -10.475953236298665\n",
      ">>>>>>>>>> 16 [5.8, 2.6, 4.0, 1.2] ; Pred= versicolor ; Actual= versicolor ;  True \n",
      "\n",
      "versicolor -1.9253351799537741\n",
      "setosa -249.60290018423612\n",
      "virginica -5.685455590035668\n",
      ">>>>>>>>>> 17 [5.6, 3.0, 4.5, 1.5] ; Pred= versicolor ; Actual= versicolor ;  True \n",
      "\n",
      "versicolor -30.413552751053114\n",
      "setosa -0.7556196570674242\n",
      "virginica -46.34802054571772\n",
      ">>>>>>>>>> 18 [5.4, 3.4, 1.5, 0.4] ; Pred= setosa ; Actual= setosa ;  True \n",
      "\n",
      "versicolor -39.332698888393494\n",
      "setosa -0.15111228928023512\n",
      "virginica -55.258412720860115\n",
      ">>>>>>>>>> 19 [5.0, 3.2, 1.2, 0.2] ; Pred= setosa ; Actual= setosa ;  True \n",
      "\n",
      "versicolor -1.3880543088253825\n",
      "setosa -209.43701918241192\n",
      "virginica -9.419562441919206\n",
      ">>>>>>>>>> 20 [5.5, 2.6, 4.4, 1.2] ; Pred= versicolor ; Actual= versicolor ;  True \n",
      "\n",
      "versicolor -2.2409904036757404\n",
      "setosa -248.6267533952453\n",
      "virginica -6.203908758687335\n",
      ">>>>>>>>>> 21 [5.4, 3.0, 4.5, 1.5] ; Pred= versicolor ; Actual= versicolor ;  True \n",
      "\n",
      "versicolor -5.359437586202645\n",
      "setosa -349.0227263062297\n",
      "virginica -2.33683306650311\n",
      ">>>>>>>>>> 22 [6.7, 3.0, 5.0, 1.7] ; Pred= virginica ; Actual= versicolor ;  False \n",
      "\n",
      "versicolor -37.06244522290436\n",
      "setosa 0.5722622258804022\n",
      "virginica -52.951342710986815\n",
      ">>>>>>>>>> 23 [5.0, 3.5, 1.3, 0.3] ; Pred= setosa ; Actual= setosa ;  True \n",
      "\n",
      "versicolor -14.695334112175255\n",
      "setosa -526.4818759887595\n",
      "virginica -2.743074702763753\n",
      ">>>>>>>>>> 24 [7.2, 3.2, 6.0, 1.8] ; Pred= virginica ; Actual= virginica ;  True \n",
      "\n",
      "versicolor -0.9194932042391687\n",
      "setosa -186.4942859392572\n",
      "virginica -8.703508224656774\n",
      ">>>>>>>>>> 25 [5.7, 2.8, 4.1, 1.3] ; Pred= versicolor ; Actual= versicolor ;  True \n",
      "\n",
      "versicolor -44.268411809321776\n",
      "setosa -2.6648383351163787\n",
      "virginica -58.85557175039779\n",
      ">>>>>>>>>> 26 [5.5, 4.2, 1.4, 0.2] ; Pred= setosa ; Actual= setosa ;  True \n",
      "\n",
      "versicolor -36.67994830038669\n",
      "setosa 0.3705988062353285\n",
      "virginica -52.135441518064816\n",
      ">>>>>>>>>> 27 [5.1, 3.8, 1.5, 0.3] ; Pred= setosa ; Actual= setosa ;  True \n",
      "\n",
      "versicolor -1.4568748191287013\n",
      "setosa -248.62595103038046\n",
      "virginica -6.831996600019302\n",
      ">>>>>>>>>> 28 [6.1, 2.8, 4.7, 1.2] ; Pred= versicolor ; Actual= versicolor ;  True \n",
      "\n",
      "versicolor -6.935750313052443\n",
      "setosa -373.65440225642624\n",
      "virginica -2.889856103657743\n",
      ">>>>>>>>>> 29 [6.3, 2.5, 5.0, 1.9] ; Pred= virginica ; Actual= virginica ;  True \n",
      "\n",
      "29 29 100.0 %\n"
     ]
    }
   ],
   "source": [
    "i=-1\n",
    "rsArr=[]\n",
    "j=0\n",
    "for item in testX:\n",
    "    i=i+1\n",
    "    pred=predictBayes(item,paras)\n",
    "    rs=(pred==testY[i])\n",
    "    rsArr.append(rs)\n",
    "    if rs==True:\n",
    "        j+=1\n",
    "    print('>>>>>>>>>>',i,item, '; Pred=',pred, '; Actual=', testY[i], '; ', rs, '\\n')\n",
    "print(i,j, round(j/i,2)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 完全正确！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
